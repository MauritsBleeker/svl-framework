dataset:
  name: 'coco'
  root:  "./src/datasets"
  img_path: "./src/datasets/coco/coco_images"
  annotation_file: 'dataset_coco.json'
  annotation_path: "./src/datasets/annotations"
  captions_per_image: 5
  latent_target_file: 'coco_latent_targets.p'
  vocab_path: './src/vocab'
  vocab_file: 'coco_vocab.pkl'

experiment:
  wandb_project: 'contrastive-shortcuts-dev'
  entity: '<wand entity>'
  experiment_name: 'dev'
  wandb_dir: './src/out'
  out_dir: './src/out'
  cache_dir: './src/out/pre_trained_models'
  development: True
  store_best_validation_model: False

dataloader:
  batch_size: 32
  eval_batch_size: 32
  num_workers: 0
  crop_size: 224

criterion:
  name: 'infonce'  # [infonce, triplet]
  temperature: 0.07 # If None, use temperature from clip
  tune_temperature: False
  alpha: 0.2
  reconstruction_metric: 'cosine'
  ifm: # implicit feature modification (IFM)
    use_ifm: False
    epsilon: 0.1

reconstruction_constraint:
    use_constraint: False
    alpha: 0.90
    bound:  0.1
    start_val: 1.
    max: 100.

model:
  image_caption_encoder:
    name: "clip" # [clip, VSE, BASE]
    model_name: "RN50" # resnet152: VSE/BASE | RN50 / ViT-B/32 : clip
    train: True
    train_img_encoder: False
    train_cap_encoder: True # not used at the moment
    embed_dim: 1024
    word_dim: 300
    num_gru_layers: 1
    img_pooling: 'avg' # [attention, avg]
  target_decoder:
    decode_target: False
    reconstruction_dim: 768
    hidden_features: 1024

training:
  model_save_path: model_last.pth
  best_model_file: model_best.pth
  n_epochs: 5
  log_step: 10
  grad_clip: 1 # the initial value was 1.
  val_epochs: 1
  use_fp16: False

optimizer:
  name: 'adamw'
  learning_rate: 2e-5
  weight_decay: 0.0
  warmup_steps: 50

shortcuts:
  use_shortcuts: True
  n_digits: 6 # represents the number of images
  random_number: False
  training:
    on_image: True
    on_caption: True
  evaluation:
    on_image: False
    on_caption: False
  bits:
    use_bits: False
    n_bits: 8 # matches batch size of 256
    random: True

lr_scheduler:
    name: 'stepLR' #[stepLR, cosine_annealing]
    step_size: 15
    gamma: 0.1